{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install pettingzoo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0w7DKObihh9",
        "outputId": "778dbb53-ce80-46c2-c21b-860050724a07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.26.4)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 pettingzoo-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame opencv-python numpy\n",
        "!pip install git+https://github.com/Farama-Foundation/MAgent2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k579iOSDiyi7",
        "outputId": "e1e254ac-adb2-46c3-ab66-bb8e49e5ef60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting git+https://github.com/Farama-Foundation/MAgent2.git\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2.git to /tmp/pip-req-build-8dy7zwmu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2.git /tmp/pip-req-build-8dy7zwmu\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2.git to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Collecting pettingzoo>=1.23.1 (from magent2==0.3.3)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: magent2\n",
            "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696114 sha256=8082bf458af44aa27ff18191081af09db60f44e3b939e2782a4fd69e5693c132\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rkgch6u3/wheels/70/5b/62/1f5a84ce954b2b4c83166f66d69c4465dbcc89d4191b599715\n",
            "Successfully built magent2\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo, magent2\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 magent2-0.3.3 pettingzoo-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wbaxl7I8eOlD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from magent2.environments import battle_v4\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Một mạng nơ-ron đơn giản với 2 lớp fully connected\n",
        "        self.fc1 = nn.Linear(np.prod(observation_shape), 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the observation\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        self.q_network = QNetwork(observation_shape, action_shape)\n",
        "        self.target_network = QNetwork(observation_shape, action_shape)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.1\n",
        "        self.learning_rate = 1e-3\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def act(self, observation):\n",
        "        # Đổi observation thành tensor\n",
        "        state = torch.Tensor(observation).float().unsqueeze(0)\n",
        "        if random.random() < self.epsilon:\n",
        "            # Chọn hành động ngẫu nhiên\n",
        "            action = random.choice(range(self.q_network.fc3.out_features))\n",
        "        else:\n",
        "            # Chọn hành động tốt nhất (theo Q-value)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "        return action\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Lưu trữ trải nghiệm vào bộ nhớ\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        # Chọn một batch từ bộ nhớ\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.Tensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        next_states = torch.Tensor(next_states)\n",
        "        dones = torch.Tensor(dones)\n",
        "\n",
        "        # Tính giá trị Q target\n",
        "        with torch.no_grad():\n",
        "            target_q_values = self.target_network(next_states)\n",
        "            max_target_q_values = torch.max(target_q_values, dim=1)[0]\n",
        "            target = rewards + self.gamma * max_target_q_values * (1 - dones)\n",
        "\n",
        "        # Tính giá trị Q hiện tại\n",
        "        q_values = self.q_network(states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Tính loss\n",
        "        loss = torch.mean((q_value - target) ** 2)\n",
        "\n",
        "        # Cập nhật trọng số của mạng Q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Cập nhật target network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.q_network.state_dict(), filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_episodes=100, batch_size=32):\n",
        "    env = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n",
        "    # Initialize agent for red team\n",
        "    observation_shape = env.observation_space(\"red_0\").shape\n",
        "    action_shape = env.action_space(\"red_0\").n\n",
        "    agent = DQNAgent(observation_shape, action_shape)\n",
        "    #agent.q_network.load_state_dict(torch.load('blue.pt', map_location=torch.device('cpu')))\n",
        "    #agent.target_network.load_state_dict(torch.load('blue.pt', map_location=torch.device('cpu')))\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        env.reset()\n",
        "        infos = {}\n",
        "        random_iters = random.sample(range(0, 80), 9)\n",
        "        for agent_id in env.agent_iter():\n",
        "            observation, reward, termination, truncation, info = env.last()\n",
        "            # Handle terminated or truncated agents\n",
        "            if termination or truncation:\n",
        "                action = None\n",
        "            else:\n",
        "                agent_handle = agent_id.split(\"_\")[0]\n",
        "                iter = int(agent_id.split(\"_\")[1])\n",
        "                #random_iter = random.randint(0, 80)\n",
        "                if agent_handle == 'blue' and iter == 0:\n",
        "                    # Get action from our trained agent\n",
        "                    action = agent.act(observation)\n",
        "                    # Store experience in replay buffer\n",
        "                    #print('len',len(env.agents))\n",
        "                    if len(env.agents) > 0:  # Make sure we have a valid previous state\n",
        "                        prev_observation = infos.get('prev_observation')\n",
        "                        if prev_observation is not None:\n",
        "                            agent.remember(prev_observation,\n",
        "                                         infos.get('prev_action'),\n",
        "                                         reward,\n",
        "                                         observation,\n",
        "                                         termination or truncation)\n",
        "                    infos.update({\n",
        "                        'prev_observation': observation,\n",
        "                        'prev_action': action\n",
        "                      })\n",
        "                    # Train the agent\n",
        "                    if len(agent.memory) == 5000:\n",
        "                      print(len(agent.memory))\n",
        "                    if len(agent.memory) > batch_size:\n",
        "                        loss = agent.replay()\n",
        "                        if loss is not None:\n",
        "                            episode_losses.append(loss)\n",
        "\n",
        "                    episode_reward += reward\n",
        "                elif agent_handle == 'red':\n",
        "                    # Random actions for blue team\n",
        "                    action = env.action_space(agent_id).sample()\n",
        "\n",
        "            env.step(action)\n",
        "        # Save model if we got better results\n",
        "        if reward > best_reward:\n",
        "          best_reward = episode_reward\n",
        "          torch.save(agent.q_network.state_dict(), 'blue.pt')"
      ],
      "metadata": {
        "id": "x-WbIXxhu5a8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxkQMpFYvU1v",
        "outputId": "3fd16c41-1487-421d-e9fc-189366abf870"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]<ipython-input-6-3e3c6535a541>:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.Tensor(states)\n",
            "  5%|▌         | 5/100 [03:01<55:24, 34.99s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [1:03:36<00:00, 38.16s/it]\n"
          ]
        }
      ]
    }
  ]
}