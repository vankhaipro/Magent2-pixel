{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0w7DKObihh9",
        "outputId": "778dbb53-ce80-46c2-c21b-860050724a07"
      },
      "outputs": [],
      "source": [
        "#!pip install pettingzoo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from magent2.environments import battle_v4\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork_pre(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, action_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        x = self.cnn(x)\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        return self.network(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wbaxl7I8eOlD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            # nn.LayerNorm(120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            # nn.LayerNorm(84),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.last_layer = nn.Linear(84, action_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        x = self.cnn(x)\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        x = self.network(x)\n",
        "        self.last_latent = x\n",
        "        return self.last_layer(x)\n",
        "class DQNAgent:\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        self.q_network = QNetwork(observation_shape, action_shape)\n",
        "        self.target_network = QNetwork(observation_shape, action_shape)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.1\n",
        "        self.learning_rate = 1e-3\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def act(self, observation):\n",
        "        # Đổi observation thành tensor với định dạng (channels, height, width)\n",
        "        state = torch.Tensor(observation).float().permute(2, 0, 1).unsqueeze(0)\n",
        "        if random.random() < self.epsilon:\n",
        "            # Chọn hành động ngẫu nhiên\n",
        "            action = random.choice(range(self.q_network.last_layer.out_features))\n",
        "        else:\n",
        "            # Chọn hành động tốt nhất (theo Q-value)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "        return action\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Lưu trữ trải nghiệm vào bộ nhớ replay.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Chọn một batch từ bộ nhớ\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        # Xử lý dữ liệu để phù hợp với kiến trúc CNN\n",
        "        states = torch.Tensor(states).permute(0, 3, 1, 2)\n",
        "        next_states = torch.Tensor(next_states).permute(0, 3, 1, 2)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        dones = torch.Tensor(dones)\n",
        "\n",
        "        # Tính giá trị Q target\n",
        "        with torch.no_grad():\n",
        "            target_q_values = self.target_network(next_states)\n",
        "            max_target_q_values = torch.max(target_q_values, dim=1)[0]\n",
        "            target = rewards + self.gamma * max_target_q_values * (1 - dones)\n",
        "\n",
        "        # Tính giá trị Q hiện tại\n",
        "        q_values = self.q_network(states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Tính loss\n",
        "        loss = torch.mean((q_value - target) ** 2)\n",
        "        \n",
        "        # Cập nhật trọng số của mạng Q\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Cập nhật target network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x-WbIXxhu5a8"
      },
      "outputs": [],
      "source": [
        "def train(num_episodes=100, batch_size=32):\n",
        "    env = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n",
        "    # env = battle_v4.env(map_size=45, minimap_mode=False, step_reward=-0.005,\n",
        "    # dead_penalty=-0.1, attack_penalty=-0.1, attack_opponent_reward=0.2,\n",
        "    # max_cycles=350, extra_features=False)\n",
        "    observation_shape = env.observation_space(\"red_0\").shape\n",
        "    action_shape = env.action_space(\"red_0\").n\n",
        "    agent = DQNAgent(observation_shape, action_shape)\n",
        "    best_reward = float('-inf')\n",
        "    q_network_red = QNetwork_pre(\n",
        "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
        "    )\n",
        "    q_network_red.load_state_dict(\n",
        "        torch.load('red.pt', weights_only=True, map_location=\"cpu\")\n",
        "    )\n",
        "\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        env.reset()\n",
        "        infos = {}\n",
        "        random_iters = random.sample(range(0, 80), 9)\n",
        "\n",
        "        for agent_id in env.agent_iter():\n",
        "            observation, reward, termination, truncation, info = env.last()\n",
        "            # Chuyển đổi quan sát thành định dạng phù hợp với CNN\n",
        "            if observation is not None:\n",
        "                observation = np.array(observation)  # Chuyển thành numpy array nếu cần\n",
        "            \n",
        "            if termination or truncation:\n",
        "                action = None\n",
        "            else:\n",
        "                agent_handle = agent_id.split(\"_\")[0]\n",
        "                iter = int(agent_id.split(\"_\")[1])\n",
        "                if agent_handle == 'blue' :\n",
        "                    if agent_id == infos.get('prev_agent'):\n",
        "                        action = agent.act(observation)\n",
        "                        if len(env.agents) > 0:  # Lưu trải nghiệm vào replay buffer\n",
        "                            # prev_observation = infos.get('prev_observation')\n",
        "                            env.step(action)\n",
        "                            prev_observation,_,_,_,_,= env.last()\n",
        "                            if prev_observation is not None:\n",
        "                                agent.remember(observation,\n",
        "                                           infos.get('prev_action'),\n",
        "                                           reward,\n",
        "                                           prev_observation,\n",
        "                                           termination or truncation)\n",
        "                    infos.update({\n",
        "                        'prev_observation': observation,\n",
        "                        'prev_action': action,\n",
        "                        'prev_agent':agent_id\n",
        "                    })\n",
        "                    if len(agent.memory) > batch_size:\n",
        "                        loss = agent.replay()\n",
        "                        if loss is not None:\n",
        "                            episode_losses.append(loss)\n",
        "\n",
        "                    episode_reward += reward\n",
        "                elif agent_handle == 'red':\n",
        "                    action = env.action_space(agent_id).sample()\n",
        "            env.step(action)\n",
        "        if reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            torch.save(agent.q_network.state_dict(), 'blue.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxkQMpFYvU1v",
        "outputId": "3fd16c41-1487-421d-e9fc-189366abf870"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [06:08<00:00,  7.37s/it]\n"
          ]
        }
      ],
      "source": [
        "train(num_episodes=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
